{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNnqKy+uDWCJ8FQ9D09gNeE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushalx05/fakenewsdetection/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# Task\n",
        "write a pytorch model for fake news prediction for the csv files ive given u..preprocess the data and use dataloaders and use efficient model to reduce the looss function\n",
        "\n",
        "Here is all the data you need:\n",
        "\"True.csv\"\n",
        "\"Fake.csv\""
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "FQDiTqmD4Lo2"
      }
    },
    {
      "source": [
        "## Data loading\n",
        "\n",
        "### Subtask:\n",
        "Load the \"True.csv\" and \"Fake.csv\" files into separate pandas DataFrames.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "GTZvt7y34L4u"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Load the \"True.csv\" and \"Fake.csv\" files into pandas DataFrames.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "CFaDQVfv4MIj"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_true = pd.read_csv('True.csv')\n",
        "df_fake = pd.read_csv('Fake.csv')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "YM_6mTW34MYh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "source": [
        "## Data preparation\n",
        "\n",
        "### Subtask:\n",
        "Create PyTorch Datasets and DataLoaders for training, validation, and testing sets.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv04cf8L4Q-W"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Create a custom PyTorch Dataset class, instantiate three dataset objects for training, validation, and testing sets, and create PyTorch DataLoaders for each dataset.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "LyCIWrfl4Sgf"
      }
    },
    {
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Concatenate the true and fake dataframes\n",
        "df_true['label'] = 1\n",
        "df_fake['label'] = 0\n",
        "df = pd.concat([df_true, df_fake], ignore_index=True)\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X = df['text']\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
        "\n",
        "# Define the custom PyTorch dataset class\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts.iloc[idx]\n",
        "        label = self.labels.iloc[idx]\n",
        "        return text, label\n",
        "\n",
        "# Create dataset objects for training, validation, and testing\n",
        "train_dataset = NewsDataset(X_train, y_train)\n",
        "val_dataset = NewsDataset(X_val, y_val)\n",
        "test_dataset = NewsDataset(X_test, y_test)\n",
        "\n",
        "# Create DataLoaders for each dataset\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Print some examples from each dataset and DataLoader\n",
        "print(f\"Train Dataset Example: {train_dataset[0]}\")\n",
        "print(f\"Val Dataset Example: {val_dataset[0]}\")\n",
        "print(f\"Test Dataset Example: {test_dataset[0]}\")\n",
        "\n",
        "print(\"Training DataLoader Example:\")\n",
        "for batch_idx, (texts, labels) in enumerate(train_dataloader):\n",
        "    print(f\"Batch {batch_idx}: Text shape: {len(texts)}, Label shape: {len(labels)}\")\n",
        "    break"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd0ZbiTQ4SwO",
        "outputId": "66d1d288-047b-434d-c02c-9729a384cee1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Example: ('BRASILIA (Reuters) - Brazil s top prosecutor on Friday charged six lawmakers from President Michel Temer s Brazilian Democracy Movement Party (PMDB) with forming a criminal organization, the latest in a barrage of charges in the country s sprawling corruption scandal. Those accused by prosecutor Rodrigo Janot in a filing with the Supreme Court include former senator and president Jose Sarney, the government s leader in the Senate Romero Juca and four other current senators. A corruption scandal involving cartels of companies bribing officials for public contracts has enveloped most of Brazil s political elite with Janot expected to issue another charge against Temer in coming weeks.  Temer defeated a first corruption charge from Janot, when the lower house of Congress voted not to allow it to proceed to trial. Separately on Friday, the Supreme Court said it would consider next week requests from Temer that would block Janot from issuing further charges. In the charges against the senators, Janot alleges the group  unduly received 864 million reais ($279.84 million) and generated related losses of 5.5 billion reais for state-controlled oil firm Petrobras and 113 million reais for its subsidiary Transpetro. Transpetro s former president, Sergio Machado, was also charged. The PMDB said in a statement that the prosecutor lacked evidence and that it trusted the Supreme Court would set the charges aside. Juca said in a statement he believes the Supreme Court will seriously analyze the charges and he hopes for a speedy investigation. A representative for Machado said he continues to collaborate with authorities, providing evidence of crimes involving politicians and Transpetro suppliers that resulted in several cases being filed. Janot s latest charges echo those against former presidents Luiz Inacio Lula da Silva, Dilma Rousseff and six other members of the Workers Party for criminal organization earlier this week. Lula and Rousseff deny the charges. ($1 = 3.0875 reais) ', np.int64(1))\n",
            "Val Dataset Example: ('SAN FRANCISCO (Reuters) - The U.S. Homeland Security Department’s inspector general said on Friday he was investigating possible abuse of authority in a case that triggered a lawsuit against the department by Twitter Inc (TWTR.N). Inspector General John Roth described the probe in a letter to Senator Ron Wyden, an Oregon Democrat who had asked for an investigation due to concerns about free speech protections.  In a lawsuit on April 6, Twitter disclosed that it received a summons in March from the U.S. Bureau of Customs and Border Protection, an agency within Homeland Security, demanding records about an account on the social media platform identified by the handle @ALT_uscis. The account has featured posts critical of President Donald Trump’s immigration policies, leading Twitter to complain in  its lawsuit that the summons was an unlawful attempt to suppress dissent. The agency dropped its demand of Twitter the day after the suit was filed. Customs bureau spokesman Mike Friel said on Friday that the bureau requested the inspector general’s review and will fully support it. The people behind the Twitter account have not disclosed their identities, but the use of “ALT” with a government agency acronym has led many to assume government employees were behind the tweets critical of Trump.  The lawsuit said the account “claims to be” the work of at least one federal immigration employee. USCIS is the acronym of United States Citizenship and Immigration Services, a component of Homeland Security. Roth’s office is charged with investigating waste, fraud and abuse within Homeland Security. He wrote in his letter that he was looking at whether the summons to Twitter “was improper in any way, including whether CBP abused its authority.” “DHS OIG is also reviewing potential broader misuse of summons authority at the department,” he added. Wyden’s office posted the letter online. A representative for Roth could not immediately be reached for comment. A Twitter spokeswoman declined to comment. ', np.int64(1))\n",
            "Test Dataset Example: ('Donald Trump s White House is in chaos, and they are trying to cover it up. Their Russia problems are mounting by the hour, and they refuse to acknowledge that there are problems surrounding all of this. To them, it s  fake news,  or a  hoax.  However, the facts bear things out differently, and it seems that there are now cracks in the Congressional public leadership.Chuck Grassley (R-Iowa), who heads the Senate Judiciary Committee, is fed up. He is now demanding that Donald Trump, Jr. and former 2016 Trump Campaign Manager Paul Manafort testify before his committee regarding the now infamous shady meeting between Donald Trump and the shady Russian lawyer who promised dirt on 2016 Democratic Presidential nominee Hillary Clinton. In fact, this information is due, well, NOW. This demand sends a few signals to team Trump   most notably that they should not fire Special Counsel Robert Mueller under any circumstances, despite the fact that it seems that this seems to be what Trump s White House is laying the groundwork, so to speak, to do as we speak.Here is the tweet regarding Grassley s warning:Also, anyone who thinks that Senator Grassley and the rest of the Senate are not serious about this only needs to look at the warning that has already been given: Trump Jr. and Manafort will either follow orders, or be served with subpoenas that force them to comply. If they refuse, they will be held in contempt of Congress, which carries with it serious jail time.Even the cruel, craven creatures within the GOP are sick of Donald Trump s corruption and his scandal-ridden White House. They are angry that he staged a hostile takeover of their party, first with birtherism and giving them a permanently racist label all while decimating all efforts that were made to pretend the Republican Party isn t a hotbed of racism, and while turning their worlds upside down, and with it, the nation. It seems that old-timers like Grassley, who are clearly sick of Trump s bullshit, just might be the ones who could save the republic. All they need is a bit of courage.Featured image via Win McNamee/Getty Images', np.int64(0))\n",
            "Training DataLoader Example:\n",
            "Batch 0: Text shape: 32, Label shape: 32\n"
          ]
        }
      ]
    },
    {
      "source": [
        "## Feature engineering\n",
        "\n",
        "### Subtask:\n",
        "Convert the preprocessed text data into numerical features using TF-IDF.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "jRhZlOBU4XyD"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Import TfidfVectorizer and convert the text data into numerical features using TF-IDF.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "k3YoEoeZ4Yng"
      }
    },
    {
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import torch\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "# Fit the vectorizer on the training data\n",
        "vectorizer.fit(X_train)\n",
        "\n",
        "# Transform the training, validation, and test data into TF-IDF feature vectors\n",
        "X_train_tfidf = vectorizer.transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Convert the TF-IDF feature vectors into PyTorch tensors\n",
        "X_train_tfidf = torch.from_numpy(X_train_tfidf.toarray()).float()\n",
        "X_val_tfidf = torch.from_numpy(X_val_tfidf.toarray()).float()\n",
        "X_test_tfidf = torch.from_numpy(X_test_tfidf.toarray()).float()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8AETbwab4Y3R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train a feedforward neural network model for fake news prediction.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg2fTpKd4jiA"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Define and train a feedforward neural network model using the prepared data.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "YpZqTnJv4k-p"
      }
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the neural network model\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(FFNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Define model hyperparameters\n",
        "input_size = X_train_tfidf.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Create the model, loss function, and optimizer\n",
        "model = FFNN(input_size, hidden_size, output_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (texts, labels) in enumerate(train_dataloader):\n",
        "        # Convert labels to PyTorch tensors\n",
        "        labels = labels.float().unsqueeze(1)\n",
        "        # Convert text data to PyTorch tensors\n",
        "        texts_tfidf = vectorizer.transform(texts)\n",
        "        texts_tfidf = torch.from_numpy(texts_tfidf.toarray()).float()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(texts_tfidf)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print training progress\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{batch_idx + 1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Evaluation on the validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for texts, labels in val_dataloader:\n",
        "            # Convert labels to PyTorch tensors\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "            # Convert text data to PyTorch tensors\n",
        "            texts_tfidf = vectorizer.transform(texts)\n",
        "            texts_tfidf = torch.from_numpy(texts_tfidf.toarray()).float()\n",
        "\n",
        "            outputs = model(texts_tfidf)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {accuracy:.2f}%')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKeJPmXX4lOV",
        "outputId": "fc1b654d-8873-4019-cfa5-56cceefef1a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/842], Loss: 0.2280\n",
            "Epoch [1/10], Step [200/842], Loss: 0.1219\n",
            "Epoch [1/10], Step [300/842], Loss: 0.0363\n",
            "Epoch [1/10], Step [400/842], Loss: 0.0727\n",
            "Epoch [1/10], Step [500/842], Loss: 0.1408\n",
            "Epoch [1/10], Step [600/842], Loss: 0.0538\n",
            "Epoch [1/10], Step [700/842], Loss: 0.0310\n",
            "Epoch [1/10], Step [800/842], Loss: 0.0987\n",
            "Epoch [1/10], Validation Accuracy: 98.65%\n",
            "Epoch [2/10], Step [100/842], Loss: 0.0183\n",
            "Epoch [2/10], Step [200/842], Loss: 0.0287\n",
            "Epoch [2/10], Step [300/842], Loss: 0.0096\n",
            "Epoch [2/10], Step [400/842], Loss: 0.0087\n",
            "Epoch [2/10], Step [500/842], Loss: 0.0046\n",
            "Epoch [2/10], Step [600/842], Loss: 0.0088\n",
            "Epoch [2/10], Step [700/842], Loss: 0.0180\n",
            "Epoch [2/10], Step [800/842], Loss: 0.0120\n",
            "Epoch [2/10], Validation Accuracy: 98.81%\n",
            "Epoch [3/10], Step [100/842], Loss: 0.0103\n",
            "Epoch [3/10], Step [200/842], Loss: 0.0086\n",
            "Epoch [3/10], Step [300/842], Loss: 0.0032\n",
            "Epoch [3/10], Step [400/842], Loss: 0.0131\n",
            "Epoch [3/10], Step [500/842], Loss: 0.0005\n",
            "Epoch [3/10], Step [600/842], Loss: 0.0025\n",
            "Epoch [3/10], Step [700/842], Loss: 0.0069\n",
            "Epoch [3/10], Step [800/842], Loss: 0.1115\n",
            "Epoch [3/10], Validation Accuracy: 98.93%\n",
            "Epoch [4/10], Step [100/842], Loss: 0.0034\n",
            "Epoch [4/10], Step [200/842], Loss: 0.0012\n",
            "Epoch [4/10], Step [300/842], Loss: 0.0010\n",
            "Epoch [4/10], Step [400/842], Loss: 0.0053\n",
            "Epoch [4/10], Step [500/842], Loss: 0.0017\n",
            "Epoch [4/10], Step [600/842], Loss: 0.0025\n",
            "Epoch [4/10], Step [700/842], Loss: 0.0059\n",
            "Epoch [4/10], Step [800/842], Loss: 0.0086\n",
            "Epoch [4/10], Validation Accuracy: 98.67%\n",
            "Epoch [5/10], Step [100/842], Loss: 0.0009\n",
            "Epoch [5/10], Step [200/842], Loss: 0.0005\n",
            "Epoch [5/10], Step [300/842], Loss: 0.0010\n",
            "Epoch [5/10], Step [400/842], Loss: 0.0010\n",
            "Epoch [5/10], Step [500/842], Loss: 0.0020\n",
            "Epoch [5/10], Step [600/842], Loss: 0.0001\n",
            "Epoch [5/10], Step [700/842], Loss: 0.0023\n",
            "Epoch [5/10], Step [800/842], Loss: 0.0010\n",
            "Epoch [5/10], Validation Accuracy: 98.91%\n",
            "Epoch [6/10], Step [100/842], Loss: 0.0005\n",
            "Epoch [6/10], Step [200/842], Loss: 0.0003\n",
            "Epoch [6/10], Step [300/842], Loss: 0.0008\n",
            "Epoch [6/10], Step [400/842], Loss: 0.0005\n",
            "Epoch [6/10], Step [500/842], Loss: 0.0005\n",
            "Epoch [6/10], Step [600/842], Loss: 0.0004\n",
            "Epoch [6/10], Step [700/842], Loss: 0.0007\n",
            "Epoch [6/10], Step [800/842], Loss: 0.0006\n",
            "Epoch [6/10], Validation Accuracy: 98.90%\n",
            "Epoch [7/10], Step [100/842], Loss: 0.0001\n",
            "Epoch [7/10], Step [200/842], Loss: 0.0000\n",
            "Epoch [7/10], Step [300/842], Loss: 0.0003\n",
            "Epoch [7/10], Step [400/842], Loss: 0.0001\n",
            "Epoch [7/10], Step [500/842], Loss: 0.0003\n",
            "Epoch [7/10], Step [600/842], Loss: 0.0004\n",
            "Epoch [7/10], Step [700/842], Loss: 0.0005\n",
            "Epoch [7/10], Step [800/842], Loss: 0.0001\n",
            "Epoch [7/10], Validation Accuracy: 98.95%\n",
            "Epoch [8/10], Step [100/842], Loss: 0.0002\n",
            "Epoch [8/10], Step [200/842], Loss: 0.0002\n",
            "Epoch [8/10], Step [300/842], Loss: 0.0002\n",
            "Epoch [8/10], Step [400/842], Loss: 0.0001\n",
            "Epoch [8/10], Step [500/842], Loss: 0.0005\n",
            "Epoch [8/10], Step [600/842], Loss: 0.0002\n",
            "Epoch [8/10], Step [700/842], Loss: 0.0003\n",
            "Epoch [8/10], Step [800/842], Loss: 0.0001\n",
            "Epoch [8/10], Validation Accuracy: 98.86%\n",
            "Epoch [9/10], Step [100/842], Loss: 0.0002\n",
            "Epoch [9/10], Step [200/842], Loss: 0.0002\n",
            "Epoch [9/10], Step [300/842], Loss: 0.0001\n",
            "Epoch [9/10], Step [400/842], Loss: 0.0001\n",
            "Epoch [9/10], Step [500/842], Loss: 0.0000\n",
            "Epoch [9/10], Step [600/842], Loss: 0.0000\n",
            "Epoch [9/10], Step [700/842], Loss: 0.0003\n",
            "Epoch [9/10], Step [800/842], Loss: 0.0000\n",
            "Epoch [9/10], Validation Accuracy: 98.91%\n",
            "Epoch [10/10], Step [100/842], Loss: 0.0003\n",
            "Epoch [10/10], Step [200/842], Loss: 0.0001\n",
            "Epoch [10/10], Step [300/842], Loss: 0.0001\n",
            "Epoch [10/10], Step [400/842], Loss: 0.0008\n",
            "Epoch [10/10], Step [500/842], Loss: 0.0001\n",
            "Epoch [10/10], Step [600/842], Loss: 0.0004\n",
            "Epoch [10/10], Step [700/842], Loss: 0.0001\n",
            "Epoch [10/10], Step [800/842], Loss: 0.0003\n",
            "Epoch [10/10], Validation Accuracy: 98.89%\n"
          ]
        }
      ]
    },
    {
      "source": [
        "## Model evaluation\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model on the test dataset.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "2IuPvakl5Zcm"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained model on the test dataset using the test_dataloader and calculate accuracy, precision, recall, and F1-score.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "x57aZVe_5aUE"
      }
    },
    {
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_dataloader:\n",
        "        # Convert labels to PyTorch tensors\n",
        "        labels = labels.float().unsqueeze(1)\n",
        "        # Convert text data to PyTorch tensors\n",
        "        texts_tfidf = vectorizer.transform(texts)\n",
        "        texts_tfidf = torch.from_numpy(texts_tfidf.toarray()).float()\n",
        "\n",
        "        outputs = model(texts_tfidf)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy().tolist())\n",
        "        y_pred.extend(predicted.cpu().numpy().tolist())\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1-Score: {f1:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkCrOt-d5aj2",
        "outputId": "d47391a0-df51-488d-cc98-98cc33407a28"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9898\n",
            "Test Precision: 0.9882\n",
            "Test Recall: 0.9905\n",
            "Test F1-Score: 0.9894\n"
          ]
        }
      ]
    },
    {
      "source": [
        "## Summary:\n",
        "\n",
        "## Fake News Prediction Model Analysis Summary\n",
        "\n",
        "### 1. Q&A\n",
        "\n",
        "**Q: What is the performance of the trained model on the test dataset?**\n",
        "**A:** The model achieved high accuracy (0.9898), precision (0.9882), recall (0.9905), and F1-score (0.9894) on the test dataset, indicating strong performance in distinguishing between true and fake news.\n",
        "\n",
        "**Q: What model was used for fake news prediction?**\n",
        "**A:** A feedforward neural network (FFNN) with a single hidden layer was implemented using PyTorch.\n",
        "\n",
        "**Q: What was the validation accuracy during training?**\n",
        "**A:** The model achieved a validation accuracy of around 98.95% after 10 epochs of training.\n",
        "\n",
        "\n",
        "### 2. Data Analysis Key Findings\n",
        "\n",
        "* The model achieved a high test accuracy of **0.9898**, precision of **0.9882**, recall of **0.9905**, and F1-score of **0.9894**.\n",
        "* The model's validation accuracy during training reached approximately **98.95%** after 10 epochs.\n",
        "* TF-IDF vectorization was used to convert text data into numerical features with a maximum of **5000** features.\n",
        "* The FFNN model had a hidden layer size of **128** and used Adam optimizer with a learning rate of **0.001**.\n",
        "\n",
        "### 3. Insights or Next Steps\n",
        "\n",
        "* The developed model demonstrates strong performance in classifying fake news with high accuracy, precision, recall, and F1-score.\n",
        "* Further exploration could involve experimenting with different model architectures, hyperparameters, or feature engineering techniques to potentially improve the model's performance even further.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "j2I-j4Hg5hef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.inference_mode():\n",
        "  model.eval()\n",
        "  for texts, labels in test_dataloader:\n",
        "        # Convert labels to PyTorch tensors\n",
        "        labels = labels.float().unsqueeze(1)\n",
        "        # Convert text data to PyTorch tensors\n",
        "        texts_tfidf = vectorizer.transform(texts)\n",
        "        texts_tfidf = torch.from_numpy(texts_tfidf.toarray()).float()\n",
        "\n",
        "        outputs = model(texts_tfidf)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy().tolist())\n",
        "        y_pred.extend(predicted.cpu().numpy().tolist())\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "  print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "  print(f\"Test Precision: {precision:.4f}\")\n",
        "  print(f\"Test Recall: {recall:.4f}\")\n",
        "  print(f\"Test F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clZl23oT5nJO",
        "outputId": "88e3b2b5-40e7-4c65-bedf-abd5fce4703b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9898\n",
            "Test Precision: 0.9882\n",
            "Test Recall: 0.9905\n",
            "Test F1-Score: 0.9894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = input(\"Enter a news article: \")\n",
        "\n",
        "# Preprocess the user input using the TF-IDF vectorizer\n",
        "user_input_tfidf = vectorizer.transform([user_input])\n",
        "user_input_tfidf = torch.from_numpy(user_input_tfidf.toarray()).float()\n",
        "\n",
        "# Make a prediction\n",
        "with torch.no_grad():  # Disable gradient calculation during inference\n",
        "  model.eval()  # Set the model to evaluation mode\n",
        "  prediction = model(user_input_tfidf)\n",
        "  predicted_class = (prediction > 0.5).float().item()  # Get the predicted class (0 or 1)\n",
        "\n",
        "# Display the result\n",
        "if predicted_class == 0:\n",
        "  print(\"The news is likely to be fake.\")\n",
        "else:\n",
        "  print(\"The news is likely to be true.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQyhRoBn6Aqu",
        "outputId": "3dabb410-96d3-4f13-b32c-28284f41d7fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a news article: \"Local Government Announces New Park Development  The City Council of Anytown has approved plans for a new park to be built on the former industrial site at 123 Main Street. The park will feature walking trails, a playground, and a community garden. Construction is expected to begin next spring and be completed within a year. Funding for the project comes from a combination of city bonds and private donations.\"\n",
            "The news is likely to be fake.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5XI4M-86Lk4"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}